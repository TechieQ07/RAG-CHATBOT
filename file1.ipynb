{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEEop+pMAhR20zSg7dqmEg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import tempfile\n","import streamlit as st\n","from streamlit_chat import message\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.llms import HuggingFaceHub\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain.memory import ConversationBufferMemory\n","from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n","from langchain.embeddings import HuggingFaceEmbeddings\n","import torch\n","\n","# Function to initialize session state\n","def initialize_session_state():\n","    if 'history' not in st.session_state:\n","        st.session_state['history'] = []\n","    if 'generated' not in st.session_state:\n","        st.session_state['generated'] = [\"Hello! Ask me anything about your documents.\"]\n","    if 'past' not in st.session_state:\n","        st.session_state['past'] = [\"Hey! ðŸ‘‹\"]\n","    if 'hf_api_key' not in st.session_state:\n","        st.session_state['hf_api_key'] = None\n","\n","# Function to manage conversation flow\n","def conversation_chat(query, chain, history):\n","    result = chain({\"question\": query, \"chat_history\": history})\n","    history.append((query, result[\"answer\"]))\n","    return result[\"answer\"]\n","\n","# Display chat history and handle user input\n","def display_chat_history(chain):\n","    reply_container = st.container()\n","    container = st.container()\n","\n","    with container:\n","        with st.form(key='my_form', clear_on_submit=True):\n","            user_input = st.text_input(\"Question:\", placeholder=\"Ask about your documents\", key='input')\n","            submit_button = st.form_submit_button(label='Send')\n","\n","        if submit_button and user_input:\n","            with st.spinner('Generating response...'):\n","                output = conversation_chat(user_input, chain, st.session_state['history'])\n","            st.session_state['past'].append(user_input)\n","            st.session_state['generated'].append(output)\n","\n","    if st.session_state['generated']:\n","        with reply_container:\n","            for i in range(len(st.session_state['generated'])):\n","                message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"thumbs\")\n","                message(st.session_state['generated'][i], key=str(i), avatar_style=\"fun-emoji\")\n","\n","# Create conversational chain using a single LLM\n","def create_conversational_chain(vector_store, hf_api_key):\n","    generator_llm = HuggingFaceHub(\n","        repo_id=\"google/flan-t5-large\",\n","        model_kwargs={\"temperature\": 0.01, \"max_length\": 500},\n","        huggingfacehub_api_token=hf_api_key,\n","        task=\"text2text-generation\"  # Correct task for flan-t5\n","    )\n","\n","    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","\n","    chain = ConversationalRetrievalChain.from_llm(\n","        llm=generator_llm,\n","        chain_type=\"stuff\",\n","        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n","        memory=memory\n","    )\n","    return chain\n","\n","# Main function\n","def main():\n","    initialize_session_state()\n","    st.title(\"Multi-Docs ChatBot using HF LLM + Vector Retrieval\")\n","\n","    # User input for Hugging Face API token\n","    if not st.session_state['hf_api_key']:\n","        st.session_state['hf_api_key'] = st.text_input(\n","            \"Enter your Hugging Face API token:\", type=\"password\"\n","        )\n","        if not st.session_state['hf_api_key']:\n","            st.warning(\"Please enter your Hugging Face API token to proceed.\")\n","            return\n","\n","    st.sidebar.title(\"Document Processing\")\n","    uploaded_files = st.sidebar.file_uploader(\"Upload files\", accept_multiple_files=True)\n","\n","    if uploaded_files:\n","        text = []\n","        for file in uploaded_files:\n","            file_extension = os.path.splitext(file.name)[1].lower()\n","            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n","                temp_file.write(file.read())\n","                temp_file_path = temp_file.name\n","\n","            loader = None\n","            if file_extension == \".pdf\":\n","                loader = PyPDFLoader(temp_file_path)\n","            elif file_extension in [\".docx\", \".doc\"]:\n","                loader = Docx2txtLoader(temp_file_path)\n","            elif file_extension == \".txt\":\n","                loader = TextLoader(temp_file_path)\n","            else:\n","                st.warning(f\"Unsupported file format: {file.name}\")\n","                continue\n","\n","            try:\n","                if loader:\n","                    text.extend(loader.load())\n","            finally:\n","                os.remove(temp_file_path)\n","\n","        if not text:\n","            st.error(\"No valid documents were loaded.\")\n","            return\n","\n","        # Split the document into smaller chunks\n","        text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000, chunk_overlap=100, length_function=len)\n","        text_chunks = text_splitter.split_documents(text)\n","\n","        # Choose device based on availability\n","        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': device})\n","        vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)\n","\n","        # Create the chain object\n","        chain = create_conversational_chain(vector_store, st.session_state['hf_api_key'])\n","\n","        # Display the chat history and allow user interaction\n","        display_chat_history(chain)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"AROauhcvA0Rt"},"execution_count":null,"outputs":[]}]}